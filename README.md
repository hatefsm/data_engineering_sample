This repository contains a hands-on data engineering sample project demonstrating a modern, modular, and production-oriented pipeline design using Python, Docker, Kafka, PostgreSQL, and structured data-lake concepts.

The goal of this project is to showcase realistic data-engineering practices, not just scripts.

ğŸ¯ Project Objectives

Build an end-to-end ingestion and validation pipeline

Apply schema enforcement and data quality checks

Store data in a layered bronze structure

Keep the project clean, reproducible, and environment-independent

Follow professional repository and Git practices

ğŸ— Architecture Overview
Source CSV / DB
        â†“
Ingestion Layer (Python)
        â†“
Schema Enforcement
        â†“
Validation Rules
        â†“
Bronze Storage Layer
        â†“
Future: Silver / Gold Modeling


Current scope focuses on Bronze layer ingestion and validation.

ğŸ“ Project Structure
data_engineering_sample/
â”‚
â”œâ”€â”€ bronze_ingest.py
â”œâ”€â”€ validate_bronze.py
â”œâ”€â”€ etl_booking_ingest_validate.py
â”‚
â”œâ”€â”€ data/               # Ignored in git (local storage)
â”œâ”€â”€ .env                # Ignored (credentials)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md

âš™ï¸ Configuration

Environment variables are managed via .env:

SOURCE_DB_HOST=localhost
SOURCE_DB_PORT=5433
SOURCE_DB_NAME=sample_source
SOURCE_DB_USER=demo_user
SOURCE_DB_PASSWORD=demo_pass
SOURCE_DB_SSLMODE=prefer

BRONZE_BASE_PATH=./data/bronze
WRITER_TYPE=local

ğŸš€ Running the Pipeline

Example execution:

python etl_booking_ingest_validate.py


The pipeline will:

Load source CSV / DB data

Enforce schema

Validate records

Split valid and invalid rows

Write outputs to bronze storage

âœ… Validation Logic

Validation ensures:

Required fields are present

Data types are respected

Business constraints are applied

Invalid records are isolated for quarantine

This mirrors real production data-quality pipelines.

ğŸ§ª Design Principles

Idempotent processing

Clear separation of responsibilities

Environment-agnostic execution

Easy transition to cloud storage (S3 / Azure / GCS)

Future-ready for orchestration (Airflow / Prefect / Dagster)

ğŸ”® Future Extensions

Planned next steps:

Silver transformation layer

Gold analytics marts

Kafka streaming ingestion

dbt transformations

Cloud deployment

CI/CD integration

ğŸ‘¤ Author

Hatef Seyed Mahdavi
Data Engineering / Data Architecture

ğŸ“Œ Purpose

This repository is designed as:

A learning reference

A portfolio project

A foundation for scalable data platform design
